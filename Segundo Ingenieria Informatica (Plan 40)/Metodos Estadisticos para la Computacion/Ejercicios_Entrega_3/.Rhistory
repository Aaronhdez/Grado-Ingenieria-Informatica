tabla<-as.table(XY)
kable(tabla)
#Aplicaci鲁n del m漏todo de Mcnemar
#Mide la simetra de la matriz
resultado_mcn<-mcnemar.test(tabla)
resultado_mcn
#Como la probabilidad es baja, se puede considerar que no existe simetra entre ambos
#Por ende, admitimos la hip鲁tesis de simetra
resultado_chi2<-chisq.test(tabla)
resultado_chi2
#Al salir la probabilidad de independecia muy cercana al cero, se puede constratar
#que son dependientes ambos del trtamiento.
library(MASS)
library(vcd)
library(knitr)
library(ggplot2)
#EJERCICIO 5: Test de Kruskal-Wallis (Generaliz. de Mann y Whitney)
P<-c(15,12,10,8,9,6,10)
A<-c(7,8,9,8,7,10,9,8,7,10)
B<-c(8,9,8,6,7,8,9,8,7,6)
C<-c(10,12,10,8,9,11,10,9,8)
tratamientos_f<-factor(rep(1:4, c(length(P),length(A),length(B),length(C))),
labels = c("Placebo","Tratamiento A","Tratamiento B","Tratamiento C"))
tratamientos_v<-c(P,A,B,C)
datos_t<-as.data.frame(tratamientos_v)
datos_t[,2]<-tratamientos_f
names(datos_t)<-c("Tiempo_recuperacion", "Tipo_tratamiento")
kable(datos_t)
tabla<-table(datos_t)
kable(tabla)
g<-ggplot(data=datos_t, aes(x=Tipo_tratamiento,
y=Tiempo_recuperacion,
color=Tipo_tratamiento))
g+geom_boxplot()
g2<-g+xlab("Tipo de procedimiento")
g2<-g2+ylab("Tiempo hasta recuperaci鲁n")
g2<-g2+geom_point(aes(shape=Tipo_tratamiento))
g2
attach(datos_t)
kruskal.test(Tiempo_recuperacion,Tipo_tratamiento, datos_tratamiento)
#Al salir la propabilidad de igualdad muy baja,
#concluimos definitivamente que son diferentes entre si.
#Verificamos cual de las dos m隆s bajas es mejor, si la A o la B
wilcox.test(Tiempo_recuperacion[Tipo_tratamiento=="Tratamiento A"],
Tiempo_recuperacion[Tipo_tratamiento=="Tratamiento B"],
alternative = "two.sided")
#Ante una probabilidad de 0,25 segun el test de Wilcoxon, se puede asumir
#que ambos tratatmientos para las plantas producen resultados iguales.
library(knitr)
library(ggplot2)
library(sqldf)
library(MASS)
# Operaciones b谩sicas rectas
## Calcular residuales     residuals(modelo)
## Calcular SCE            sum(e^2)
## Calcular el s2          SCE/(n-2)
## Calcular la Varianza    sqrt(SCE/(n-2))
## Calcular el STCC        sum((growth-mean(growth))^2)
## Calcular el Coef. Det   1-(SCE/STCC)
### A mas cercano a la unidad, mejor es el ajuste
# Inferencias rectas (Dist T)
## Confint -> Informa del intervalo de confianza de la pendiente
## qt(conf, length(tannin-2))
## Tambi茅n es ofecido por el summary (ver: t-value y PR)
## Si PR es demasiado elevado, la estimaes mala.
# Predicci贸n (Dist T)
## predict(modelo, dato-nuevo...)
### Por defecto -> calcula toda la predicci贸n
### Con para una tangente concreta, realiza una predicci贸n espec铆fica
# Uso de ANOVA
## 1. Calculamos SCE y STCC
## 2. Calculamos SCR = STCC - SCE
## 3. Calcular el s2 = sum((Y-b0+b1*x))
## 4. Calcular el F = SCR/s2
## 5. Calcular la probailidad
# Calculo de Errores en al recta.
## 1. SCEpuro = Suma
## 2. SC(Sin ajuste) = SCE - SCEpuro
## 3. Calcular k <- nlevels(x_Factor)
## 4. Calcular s2_puro <- SCEpuro/(length(X)-k)
### EJERCICIO
datos<-read.table("Archivos/Aloe_Vera.txt", sep=",", dec=".")
attach(datos)
#Apartado a)
boxplot(Masa~Variedad, Variedad, col="red")
boxplot(Masa_Seca~Variedad, Variedad, col="green")
boxplot(Num_Hojas~Variedad, Variedad, col="yellow")
boxplot(Altura~Variedad, Variedad, col="lightblue")
masa_seca<-aggregate(Masa_Seca~Variedad, Variedad, mean)
masa_seca
masa<-aggregate(Masa~Variedad, Variedad, mean)
masa
#Apartado b
Barbadensis<-subset(datos, subset=(Variedad == "barbadensis"))
detach()
attach(Barbadensis)
x<-Masa
y<-Altura
n<-length(x)
plot(Altura~Masa, data = Barbadensis, ylim=c(0,length(y)))
#Apartado c
modelobar<-lm(y~x)
abline(modelobar, col="red", lwd=2)
#Apartado d
summary(modelobar)
confint(modelobar)
#Calculo manual de los errores
e<-residuals(modelobar)
SCE<-sum(e^2)
SCE
s2<-SCE/(n-2)
s2
var<-sqrt(SCE/(n-2))
var
#Apartado e
x0<-5.1
predicci贸n_masa<-predict(modelobar,newdata = data.frame(x=x0))
predicci贸n_masa
points(x0,predicci贸n_masa, pch=16,col="black")
lines(c(x0,x0), c(x0,predicci贸n_masa),col="red")
inter_prediccion<-predict(modelobar,level=0.95, newdata =
data.frame(x=x0), interval = "pred")
inter_prediccion
lines(c(x0,x0), c(inter_prediccion[2],inter_prediccion[3]),col="darkgreen")
points(c(x0,x0), c(inter_prediccion[2],inter_prediccion[3]),col="darkgreen")
#Apartado f
STCC<-sum((y-mean(y))^2)
STCC
cdd<-1-(SCE/STCC)
cdd
#Muy pr贸ximo a la unidad. muy buen ajuste
#Apartado g
x_factor<-as.factor(x)
datos2<-data.frame(x_factor,y)
detach(Barbadensis)
attach(datos2)
Y_M_F<-rep(0,length(x_factor))
for (i in 1:length(x_factor)) {
Y_M_F[i]<-mean(y[x_factor==x[i]])
}
SCEpuro <- sum((y-Y_M_F)^2)
#Error falta de ajuste
SC <- SCE-SCEpuro
SC
#Calculo del s2puro
k<-nlevels(x_factor)
s2puro<-SCEpuro/(n-k)
s2puro
fSC<-SC/(s2puro*(k-2))
fSC
1-pf(fSC,1,k-2)
#Apartado h (OPCIONAL)
library(knitr)
library(pwr)
#Mu y sigma son desconocidas, distribucion N(mu,sigma^2)
datos <- c(3.58,10.03,4.77,9.71,10.4,14.66,8.45,5.4,9.75,10.1)
alfa <- 0.05
n <- 10
#Apartado A,
# - H0 = mu >= 10
# - H1 = mu < 10
xm <- mean(datos)
sigma <- var(datos)
mu <- 10
t <- (xm-mu)/sigma/sqrt(n)
t
qt(0.05,9) # Se rechaza la hip贸tesis, pues qt es menor que t
#Apartado B
# - H0 = mu < 10,
# - H1 = mu >= 10
qt(0.95,9) # Como qt por el otro extremo es mayor que t se acepta la hipotesis nula
#Apartado C, Calculo Errores Tipo I y II y la Potencia de la prueba
# El Error tipo I es alfa = 0.05
t1<-alfa
# La potencia viene dada por 1-beta. Se puede extraer con pwr.t.test
potencia<-pwr.t.test(mu,1,0.05,type="one.sample")
potencia
# beta sale de la potencia t = 1-beta
beta <- 1-t
beta + potencia$power
library(knitr)
n1 <- 16
x1 <- 1515.60
s1 <- sqrt(61.500)
n2 <- 10
x2 <- 1298.35
s2 <- sqrt(90.201)
#Apartado A, Intervalo confianza del 95%
a<- ((s1^2/n1)+(s2^2/n2))^2
b<- (s1^2/n1)^2/(n1-1)
c<- (s2^2/n2)^2/(n2-1)
v<-a/(b+c)
izquierda005<-qt(0.025,v)
derecha005<-qt(0.975,v)
t<-(x1-x2)/sqrt((s1^2/n1)+(s2^2/n2))
#Apartado B, Intervalo de confianza del 90%
izquierda010<-qt(0.05,v)
derecha010<-qt(0.95,v)
t
izquierda005
derecha005
#Apartado C, no hay discrminaci贸n debido a que el valor v no entra en el intervalo
#Apartado D, n1 = 45, n2 = 30
n1 <- 45
n2 <- 30
a2<- (s1^2/n1+s2^2/n2)^2
b2<- (s1^2/n1)^2/(n1-1)
c2<- (s2^2/n2)^2/(n2-1)
v2<-a2/(b2+c2)
izquierda005_2<-qt(0.025,v2)
derecha005_2<-qt(0.975,v2)
t2<-(x1-x2)/sqrt((s1^2/n1)+(s2^2/n2))
t2
izquierda005_2
derecha005_2
setwd(".")
library(knitr)
# a)
datos<-c(1500.21, 880.66, 605.22, 1210.12, 2010.1, 701.30, 2060.01, 810.10,
1012.34, 1500.08, 2500.00, 917.45, 890.50, 515.01, 820.39, 1800.30,
625.12, 1002.20, 2015.22, 720.25, 1102.45, 3200.00, 1601.79, 1219.70,
1005.40, 2150.1, 623.56)
kable(datos)
# Media
mu<-mean(datos)
mu
# Desviaci贸n
sigma<-sd(datos)
sigma
# Tama帽o datos
n<-length(datos)
n
# Intervalo de confianza inferior y superior
confInf<-mu-qt(0.95, df=(n-1))*sigma/sqrt(n)
confInf
confSup<-mu+qt(0.95, df=(n-1))*sigma/sqrt(n)
confSup
# b) Explicacion.Es una prueba Xi cuadrado. SI el resultado es < 1 se considera correcto res < 1 estaba perfe la cosa,
# Ante caso contrario entonces la hipotesis se rechaza.
# (n-1)*s^2 / sigma^2 -> Prueba Xi cuadrado
a<-qchisq(0.95,26)
b<-a*a
c<-b*26
d<-1000*1000
res<-c/d
res*100
# Al ser mayor de 3 se rechaza la hip贸tesis.
library(knitr)
datos <- c(6.2,6.6,5.8,5.4,5.3,6.15,6.68,7.0,5.8,5.6,5.85,6.2,6.4,6.75,5.3,6.3)
n <- length(datos)
pro <- 5.35
media <- mean(datos)
s <- var(datos)
sd_c <-sd(datos)
#Apartado A, con alfa == 0.01,
# - H0 -> mu >= 5.35
# - H1 -> mu != 5.35
alfa <- 0.01
t <- (media-pro)/s/sqrt(n)
t
qt <- qt(0.01,15)
#Se acepta h0 puesto que la proximidad a 0 es mayor que qt.
#Apartado B, prob = 0.99.
pro2 <- 6.0
d_c <- (pro2-promedio)/s
b <- 0.99
power.t.test(n = NULL, delta = d_c,sd = sd_c,sig.level = 0.01,power = b,alternative = "one.sided")
# A una probabilidad el 99%, el power test da un tama帽o de muestra de aprox 4 miembros.
#Apartado C
b_2 <- 0.90
power.t.test(n = NULL, delta = d_c,sd = sd_c,sig.level = 0.01,power = b_2,alternative = "one.sided")
library(knitr)
Tabla<-matrix(c(35,31,47,55),2,2,byrow=TRUE)
colnames(Tabla)<-c("Hombre","Mujer"); rownames(Tabla)<-c("Interesados/as","No Interesadas/os")
Tabla<-as.table(Tabla)
kable(Tabla)
#Se distribuyen con una distribucion:
#v = (2-1)(2-1) = 1 grado de libertad
qchisq(0.95,1)
#Pruebas de homogeneidad de forma teorica
XY<-matrix(c(35,47,31,55),
ncol=2,nrow = 2)
colnames(XY)<-c("Hombre", "Mujer")
rownames(XY)<-c("Interesados/as","No Interesados/as")
XY
pXY<-matrix(c(35/66*35/82,47/102*47/82, 31/66*31/86,55/102*55/86),
ncol=2,nrow = 2)
sum(pXY)
CHI2<-sum(XY)*(sum(pXY)-1)
CHI2
# Regi贸n Cr铆tica
gl<-(nrow(XY)-1)*(ncol(XY)-1)
qchisq(0.95,gl)
CHI2<-sum(XY)*(sum(pXY)-1)
CHI2
# Regi贸n Cr铆tica
gl<-(nrow(XY)-1)*(ncol(XY)-1)
qchisq(0.95,gl)
test<-chisq.test(XY)
test
#Vemos el valor del estad铆stico de contraste (0.7750765),
#los grados de libertad (1) y el bajo p-valor 0.4701 mayor que 0.05, nos
#permite concluir que con esos datos podemos aceptar la hip贸tesis nula de homogeneidad de los terrenos.
#Los porcentajes de Interesados/as ser谩n iguales ya sea hombre o mujer
#Ahora lo hacemos a trav茅s de las pruebas de Magnemar
#Pruebas de Magnemar
result <- mcnemar.test(Tabla, correct = T)
result
# 0.08943 > 0.05 Por tanto se admite la hip贸tesis nula de existencia de simetr铆a
#El valor para 1-a de la distribuci贸n x2 con 1 grados de libertad es:
qchisq(0.95, 1)
x2 <- (47-32)^2/(47+31)
x2
#Y para x2= 2.884615 con 1 grado de libertad.
res <- 1-pchisq(2.884615,1)
res
#C贸mo 0.08942938 > 0.05 concluimos que la hip贸tesis nula se cumple, en consecuencia tambi茅n la simetr铆a
result2 <- chisq.test(Tabla, correct = TRUE)
result2
#Como 0.4701 > 0.05
setwd(".")
library(knitr)
notas_al <- c(5.7,8.6,3.6,1.5,8.8,5.9,4.9,8.6,7.6,5.0,
7.7,2.6,8.6,7.5,5.8,6.2,9.9,7.1,5.6,6.2,
7.6,6.5,6.7,4.5,4.8,6.9,8.9,2.6,5.5,7.0)#Mira tabla uminima
n1 <- length(notas_al)
notas_pr <- c(5.0,7.0,5.2,1.3,7.2,6.6,3.1,8.6,6.0,6.1,
8.0,5.0,9.2,7.3,4.2,6.6,9.1,7.6,4.0,5.1,
8.0,8.1,9.1,4.5,3.2,7.6,7.1,4.6,6.0,5.8)
n2 <- length(notas_pr)
alfa <- 0.05
#C谩culo manual
notas_al_ord <- sort(notas_al)
notas_pr_ord <- sort(notas_pr)
notas_al_ord
notas_pr_ord
# Valores de los rangos calculados a mano
R1 <- 808.5
R2 <- 849.5
U1 <- n1*n2+((n1*(n1+1))/2)-R1
U2 <- n1*n2+((n2*(n2+1))/2)-R2
U1
U2
mu_U <- (n1*n2)/2
sigma_U_2 <- (n1*n2*(n1+n2+1))/12
Z <- (U2-mu_U)/sqrt(sigma_U_2)
Z
qnorm(0.025)
qnorm(0.975)
# Como z (0.96837) se encuntra en el intervalo (-1.95, 1.95), se acepta la hip贸tesis nula, H0.
# Usando wilcox.test
# Quitamos los 0 de X-Y
resta <- notas_al - notas_pr
resta
notas_al_new <- c(5.7,8.6,3.6,1.5,8.8,5.9,4.9,7.6,5.0,
7.7,2.6,8.6,7.5,5.8,6.2,9.9,7.1,5.6,6.2,
7.6,6.5,6.7,4.8,6.9,8.9,2.6,5.5,7.0)
notas_pr_new <- c(5.0,7.0,5.2,1.3,7.2,6.6,3.1,6.0,6.1,
8.0,5.0,9.2,7.3,4.2,6.6,9.1,7.6,4.0,5.1,
8.0,8.1,9.1,3.2,7.6,7.1,4.6,6.0,5.8)
wilcox.test(notas_al_new,notas_pr_new,paired = T,exact = F)
# Como el valor de probabilidad se dispara a cais el 72% por el test de Wilcoxon, se acepta hip贸tesis nula
setwd(".")
library(knitr)
d1 <- c(92,89,86,83,77,71,62,2.6,53,40)
d2 <- c(88,85,93,79,70,87,52,84,41,64)
n <- length(T1)
#Coeficiente de Correlaci贸n de Pearson
# Manualmente, Aplicamos formula rp = (n*(sum) x*y - (sum)x * (sum)y)/sqrt((n*(sum)x^2-((suma)x)^2)*(n*(suma)y^2-((suma)y)^2))
suma_xy <- sum(d1*d2)
suma_x <- sum(d1)
suma_y <- sum(d2)
rp <- (n*suma_xy-suma_x*suma_y)/sqrt((n*49899.76-429811.36)*(n*57865-552049))
rp
#Usando la funci贸n cor
cor(d1,d2,method = c("pearson")) # Dependencia positiva/proporcional
#Coeficiente de Spearman
#Manualmente, ordenando por rangos de menor a mayor valor en cada conjunto
di_cuadrado <- (10-9)^2+(9-7)^2+(8-10)^2+(7-5)^2+(6-4)^2+
(5-8)^2 + (4-2)^2 + (1-6)^2 + (3-1)^2+ (2-3)^2
r <- 1-((6*di_cuadrado/(n*(n^2-1))))
r
# Usando la funci贸n cor
cor(d1,d2,method = c("spearman"))
setwd(".")
library(knitr)
library(ggplot2)
library(sqldf)
library(MASS)
# Operaciones b谩sicas rectas
## Calcular residuales     residuals(modelo)
## Calcular SCE            sum(e^2)
## Calcular el s2          SCE/(n-2)
## Calcular la Varianza    sqrt(SCE/(n-2))
## Calcular el STCC        sum((growth-mean(growth))^2)
## Calcular el Coef. Det   1-(SCE/STCC)
### A mas cercano a la unidad, mejor es el ajuste
# Inferencias rectas (Dist T)
## Confint -> Informa del intervalo de confianza de la pendiente
## qt(conf, length(tannin-2))
## Tambi茅n es ofecido por el summary (ver: t-value y PR)
## Si PR es demasiado elevado, la estimaes mala.
# Predicci贸n (Dist T)
## predict(modelo, dato-nuevo...)
### Por defecto -> calcula toda la predicci贸n
### Con para una tangente concreta, realiza una predicci贸n espec铆fica
# Uso de ANOVA
## 1. Calculamos SCE y STCC
## 2. Calculamos SCR = STCC - SCE
## 3. Calcular el s2 = sum((Y-b0+b1*x))
## 4. Calcular el F = SCR/s2
## 5. Calcular la probailidad
# Calculo de Errores en al recta.
## 1. SCEpuro = Suma
## 2. SC(Sin ajuste) = SCE - SCEpuro
## 3. Calcular k <- nlevels(x_Factor)
## 4. Calcular s2_puro <- SCEpuro/(length(X)-k)
### EJERCICIO
datos<-read.table("Archivos/Aloe_Vera.txt", sep=",", dec=".")
attach(datos)
#Apartado a)
boxplot(Masa~Variedad, Variedad, col="red")
boxplot(Masa_Seca~Variedad, Variedad, col="green")
boxplot(Num_Hojas~Variedad, Variedad, col="yellow")
boxplot(Altura~Variedad, Variedad, col="lightblue")
masa_seca<-aggregate(Masa_Seca~Variedad, Variedad, mean)
masa_seca
masa<-aggregate(Masa~Variedad, Variedad, mean)
masa
#Apartado b
Barbadensis<-subset(datos, subset=(Variedad == "barbadensis"))
detach()
attach(Barbadensis)
x<-Masa
y<-Altura
n<-length(x)
plot(Altura~Masa, data = Barbadensis, ylim=c(0,length(y)))
#Apartado c
modelobar<-lm(y~x)
abline(modelobar, col="red", lwd=2)
#Apartado d
summary(modelobar)
confint(modelobar)
#Calculo manual de los errores
e<-residuals(modelobar)
SCE<-sum(e^2)
SCE
s2<-SCE/(n-2)
s2
var<-sqrt(SCE/(n-2))
var
#Apartado e
x0<-5.1
predicci贸n_masa<-predict(modelobar,newdata = data.frame(x=x0))
predicci贸n_masa
points(x0,predicci贸n_masa, pch=16,col="black")
lines(c(x0,x0), c(x0,predicci贸n_masa),col="red")
inter_prediccion<-predict(modelobar,level=0.95, newdata =
data.frame(x=x0), interval = "pred")
inter_prediccion
lines(c(x0,x0), c(inter_prediccion[2],inter_prediccion[3]),col="darkgreen")
points(c(x0,x0), c(inter_prediccion[2],inter_prediccion[3]),col="darkgreen")
#Apartado f
STCC<-sum((y-mean(y))^2)
STCC
cdd<-1-(SCE/STCC)
cdd
#Muy pr贸ximo a la unidad. muy buen ajuste
#Apartado g
x_factor<-as.factor(x)
datos2<-data.frame(x_factor,y)
detach(Barbadensis)
attach(datos2)
Y_M_F<-rep(0,length(x_factor))
for (i in 1:length(x_factor)) {
Y_M_F[i]<-mean(y[x_factor==x[i]])
}
SCEpuro <- sum((y-Y_M_F)^2)
#Error falta de ajuste
SC <- SCE-SCEpuro
SC
#Calculo del s2puro
k<-nlevels(x_factor)
s2puro<-SCEpuro/(n-k)
s2puro
fSC<-SC/(s2puro*(k-2))
fSC
1-pf(fSC,1,k-2)
setwd(".")
library(knitr)
library(ggplot2)
library(sqldf)
library(MASS)
datos<-read.table("Archivos/Alturas_Estudiantes_EII.txt", dec = ".", sep=",")
#Apartado A
attach(datos)
n<-length(Alturas)
parametro<-fitdistr(Alturas,"normal")
mu<-parametro$estimate[1]
sigma<-parametro$estimate[2]
#Apartado B
hist(Alturas, freq = F, col="lightblue", ylim=c(0,0.025))
abline(v=mu, col="red", lty=2, lwd=2)
x<-seq(min(Alturas),max(Alturas),5)
points(x,dnorm(x,mu,sigma), type="l", col="blue", lwd=2)
#Apartado C
sdmu<-parametro$sd[1]
sdmu
sdsigma<-parametro$sd[2]
sdsigma
#Poca desviaci贸n en ambos casos,
#la distribuci贸n sigue una forma gaussiana
